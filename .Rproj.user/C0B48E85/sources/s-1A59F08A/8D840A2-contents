---
title: "P8106_hw2_aka2170"
author: "Ahlam Abuawad"
date: "3/9/2018"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float: yes
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(tidyverse)
library(janitor)
library(caret)
library(corrplot)
library(broom)
library(pROC)
library(glmnet)
library(MASS)
library(class)
```

## Weekly Dataset

```{r import}
attach(Weekly)

dim(Weekly) #1089
```

* The Weekly dataset contains `r nrow(Weekly)` observations and `r ncol(Weekly)` variables. 
* The dataset was obtained over a span of 21 years (2009 - 2010), where the Lag variables are the percent returns for the S&P 500 stock index during a specific lag. 
* The shares traded (averaged daily in billions) are denoted by the volume variable. 
* The outcome of direction is a two factor variable of either "Down" (negative) or "Up" (positive) depending on the market return for the week.


## (A) Summaries of Weekly Dataset

### Numerical Summaries of Dataset

```{r num sum}
# Summary of Predictors
summary(Weekly[,1:7]) %>% 
  knitr::kable()

# Summary of Outcome
summary(Weekly[,9]) %>% 
  knitr::kable()
```

### Visual Summaries of Dataset

```{r vis sum, warning = FALSE}
# Associations with Outcome and Predictors
featurePlot(x = Weekly[,1:7],
            y = Direction,
            plot = "pairs", 
            auto.key = list(columns = 2), 
            col = c("orange", "purple"), 
            overlay = FALSE)

# Correlations between Precitors
correlations = cor(Weekly[,1:7])
corrplot(correlations, order = "original")

# Inspecting Relationship between Volume and Years
plot(x = Year, y = Volume, title("Market Volume Changes from 1990 - 2010")) # index = row #
```

The correlation plot of the Weekly dataset reveals that volume and year have a strong correlation. Over the years, the volume increased, but mostly in the 2000's. 


## (B) Logistic Regression (Full Dataset)

This Model will use all Five *Lag* Variables and *Volume* as Predictors for *Direction*.

```{r log}
# Logistic Regression Model
glm.fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
            data = Weekly, family = binomial)

summary(glm.fit)
```

**Do any of the predictors appear to be statistically significant? If so, which ones?**

Out of all six predictors, only Lag 2 appears to be statistically significant with a beta coefficient value of 0.058 (p-value = 0.0296).


## (C) Confusion Matrix

```{r confuse}
# Inspecting Dummy Variable Created
contrasts(Direction) # 0 is down and 1 is up

# creating predictions from logistic regression model to predict up or down values
test_pred_prob = predict(glm.fit, type = "response") # type="response" --> R outputs prob as P(Y = 1|X)

# setting down as all values
test_pred = rep("Down", length(test_pred_prob))

# changing down to up values if closer to predicted value of 1
test_pred[test_pred_prob > 0.5] = "Up"

table(test_pred, Direction)

sensitivity(data = as.factor(test_pred),
            reference = Direction)
specificity(data = as.factor(test_pred),
            reference = Direction)

# Confusion Matrix
con_glm1 = confusionMatrix(data = as.factor(test_pred),
                reference = Direction)

accur_glm1 = mean(test_pred == Direction)
```

**Report the overall fraction of correct predictions. Briefly explain what the confusion matrix is telling you.**

The overall fraction ("Accuracy") of correct predictions is 56.1%. The confusion matrix reveals that the logistic model for the weekly direction accurately predicted the outcome 92.1% of the time when the direction was "Up" and the market had a positive return (specificity). Conversely, if the weekly direction was "Down", the logistic model accurately predicted this negative return only 11.1% of the time (sensitivity). This means that the model did not predict the correct direction of the return for 478 weeks out of the total 1089 weeks in the Weekly dataset.


## (D) ROC Curve

This ROC Curve used the Predicted Probability from Logistic Regression.

```{r ROC}
# ROC Curve
roc.glm = roc(Direction, test_pred_prob,
               levels = c("Down", "Up"))
roc.glm

plot(roc.glm, legacy.axes = TRUE)
```

**Report the area under the curve (AUC).**

The AUC from the logistic model (of all 5 Lags and Volume) fitted using the entire dataset to make predictions is equal to 0.5537.


## (E) Logistic Regression (Training Data from 1990 - 2008)

This Model used *Lag1* and *Lag2* as the Predictors. The corresponding ROC Curve used the held out data (from 2009 - 2010).

```{r log2, warning = FALSE}
# Creating Training Set
wk_train = (Year < 2009)
wk_test = Weekly[!wk_train,]

dim(wk_test) # 104

dir_910 = Direction[!wk_train]

# Logistic Regression Model
glm.fit2 = glm(Direction ~ Lag1 + Lag2, 
            data = Weekly, subset = wk_train, family = binomial)

summary(glm.fit2)

# Creating Predictions
test_pred_prob2 = predict(glm.fit2, newdata = wk_test, type = "response")

test_pred2 = rep("Down", length(test_pred_prob2))
test_pred2[test_pred_prob2 > 0.5] = "Up"

# Confusion Matrix
con_glm2 = confusionMatrix(data = as.factor(test_pred2),
                reference = dir_910)

# ROC Curve
roc.glm2 = roc(dir_910, test_pred_prob2,
               levels = c("Down", "Up"))
roc.glm2

plot(roc.glm2, legacy.axes = TRUE)
```

**Report the area under the curve (AUC).**

The AUC for this logistic regression model (of Lag 1 & 2) fitted with training set to make predictions on data from 2009-2010 is 0.5559.


## (E) Linear Discriminant Analysis (LDA)

```{r LDA}
# LDA Model
lda.fit = lda(Direction ~ Lag1 + Lag2, data = Weekly, subset = wk_train)
plot(lda.fit)

lda_pred = predict(lda.fit, newdata = wk_test)
head(lda_pred$posterior) # posterior = posterior probabilities

# Confusion Matrix
con_lda = confusionMatrix(data = as.factor(lda_pred$class),
                reference = dir_910)

# ROC Curve
roc_lda = roc(dir_910, lda_pred$posterior[,2], 
               levels = c("Down", "Up"))
roc_lda

plot(roc_lda, legacy.axes = TRUE)
```

**The AUC for this LDA model (of Lag 1 & 2) fitted with training set to make predictions on data from 2009-2010 is 0.5566.**


## (F) Quadratic Discriminant Analysis (QDA)

```{r QDA}
# QDA
qda.fit = qda(Direction ~ Lag1 + Lag2, data = Weekly, subset = wk_train)

qda_pred = predict(qda.fit, newdata = wk_test)
head(qda_pred$posterior)

# Confusion Matrix
con_qda = confusionMatrix(data = as.factor(qda_pred$class),
                reference = dir_910)

# ROC Curve
roc_qda = roc(dir_910, qda_pred$posterior[,2], # 2 is used to only look at posterior prob of "Up"
               levels = c("Down", "Up"))
roc_qda

plot(roc_qda, legacy.axes = TRUE)
```

**The AUC for this QDA model (of Lag 1 & 2) fitted with training set to make predictions on data from 2009-2010 is 0.5288.**


## (G) K-Nearest Neighbors (KNN)

```{r KNN}
# KNN
train_X = cbind(Lag1, Lag2)[wk_train,]
test_X = cbind(Lag1, Lag2)[!wk_train,]
dir_train = Direction[wk_train]

set.seed(8)
```

### K = 1

```{r KNN1}
knn.pred1 = knn(train_X, test_X, dir_train, k = 1, prob = TRUE)

table(knn.pred1, dir_910)

# Confusion Matrix
con_knn1 = confusionMatrix(data = as.factor(knn.pred1),
                reference = dir_910)

# ROC for K = 1
roc.knn1 = roc(dir_910, attr(knn.pred1,"prob"),
               levels = c("Down", "Up"))
roc.knn1

plot(roc.knn1, legacy.axes = TRUE)
```

### K = 2

```{r KNN2}
knn.pred2 = knn(train_X, test_X, dir_train, k = 2, prob = TRUE)

table(knn.pred2, dir_910)

# Confusion Matrix
con_knn2 = confusionMatrix(data = as.factor(knn.pred2),
                reference = dir_910)

# ROC for K = 2
roc.knn2 = roc(dir_910, attr(knn.pred2,"prob"),
               levels = c("Down", "Up"))
roc.knn2

plot(roc.knn2, legacy.axes = TRUE)
```

### K = 3

```{r KNN3}
knn.pred3 = knn(train_X, test_X, dir_train, k = 3, prob = TRUE)

table(knn.pred3, dir_910)

# Confusion Matrix
con_knn3 = confusionMatrix(data = as.factor(knn.pred3),
                reference = dir_910)

# ROC for K = 3
roc.knn3 = roc(dir_910, attr(knn.pred3,"prob"),
               levels = c("Down", "Up"))
roc.knn3

plot(roc.knn3, legacy.axes = TRUE)
```

### K = 4

```{r KNN4}
knn.pred4 = knn(train_X, test_X, dir_train, k = 4, prob = TRUE)

table(knn.pred4, dir_910)

# Confusion Matrix
con_knn4 = confusionMatrix(data = as.factor(knn.pred4),
                reference = dir_910)

# ROC for K = 4
roc.knn4 = roc(dir_910, attr(knn.pred4,"prob"),
               levels = c("Down", "Up"))
roc.knn4

plot(roc.knn4, legacy.axes = TRUE)
```

### K = 5

```{r KNN5}
knn.pred5 = knn(train_X, test_X, dir_train, k = 5, prob = TRUE)

table(knn.pred5, dir_910)

# Confusion Matrix
con_knn5 = confusionMatrix(data = as.factor(knn.pred5),
                reference = dir_910)

# ROC for K = 5
roc.knn5 = roc(dir_910, attr(knn.pred5,"prob"),
               levels = c("Down", "Up"))
roc.knn5

plot(roc.knn5, legacy.axes = TRUE)
```

### K = 6

```{r KNN 6}
knn.pred6 = knn(train_X, test_X, dir_train, k = 6, prob = TRUE)

table(knn.pred6, dir_910)

# Confusion Matrix
con_knn6 = confusionMatrix(data = as.factor(knn.pred6),
                reference = dir_910)

# ROC for K = 6
roc.knn6 = roc(dir_910, attr(knn.pred6,"prob"),
               levels = c("Down", "Up"))
roc.knn6

plot(roc.knn6, legacy.axes = TRUE)
```

### KNN Comparisons

```{r all ks}
# Accuracies for all KNN Models
knn_accur = cbind(as.vector(con_knn1$overall["Accuracy"]), as.vector(con_knn2$overall["Accuracy"]), as.vector(con_knn3$overall["Accuracy"]), as.vector(con_knn4$overall["Accuracy"]), as.vector(con_knn5$overall["Accuracy"]), as.vector(con_knn6$overall["Accuracy"]))

rownames(knn_accur) = colnames(con_knn1$overall["Accuracy"]); colnames(knn_accur) = c("K = 1","K = 2","K = 3", "K = 4", "K = 5", "K = 6")
knitr::kable(knn_accur, align = "c")

# AUC for all KNN Models
knn_aucs = cbind(as.vector(roc.knn1$auc), as.vector(roc.knn2$auc), as.vector(roc.knn3$auc), as.vector(roc.knn4$auc), as.vector(roc.knn5$auc), as.vector(roc.knn6$auc))

rownames(knn_aucs) = colnames(roc.knn1$auc); colnames(knn_aucs) = c("K = 1","K = 2","K = 3", "K = 4", "K = 5", "K = 6")
knitr::kable(knn_aucs, align = "c")
```

**Briefly discuss your results.**

For the KNN models, K = 3-Nearest Neighbors had one of the highest AUCs (0.5799), but also had the highest overall fraction of correct predictions (0.5192) compared to the other five K values tested. Thus, this was the optimal KNN model developed using the training set from 1990-2008. 

## Comparison of All Models

### Accuracies for all Models

```{r accur}
accur = cbind(as.vector(con_glm1$overall["Accuracy"]), as.vector(con_glm2$overall["Accuracy"]), as.vector(con_lda$overall["Accuracy"]), as.vector(con_qda$overall["Accuracy"]), as.vector(con_knn3$overall["Accuracy"]))

rownames(accur) = colnames(con_glm1$overall["Accuracy"]); colnames(accur) = c("Logistic Reg","Logistic Reg (w. Training Set)", "LDA", "QDA", "KNN")
knitr::kable(accur, align = "c")
```

Surprisingly, the logistic regression model using the training set and the LDA had identical overall fractions of correct predictions (0.5769). The model with the lowest overall fraction of correct predictions was the KNN with a value of 0.5192.


### AUCs for all Models

```{r AUC all}
aucs = cbind(as.vector(roc.glm$auc), as.vector(roc.glm2$auc), as.vector(roc_lda$auc), as.vector(roc_qda$auc), as.vector(roc.knn3$auc))

rownames(aucs) = colnames(roc.glm$auc); colnames(aucs) = c("Logistic Reg","Logistic Reg (w. Training Set)","LDA", "QDA", "KNN")
knitr::kable(aucs, align = "c")
```

However, out of all of the models tested, KNN had by far the largest AUC (0.5799) for its corresponding ROC Curve. As a result, this model balanced the sensitivity against the 1 - specificity the best out of all the models tested. The model that had the worst balance was the QDA, which had the lowest AUC of 0.5287. 


### Sensitivities for all Models

```{r sens all}
sens = cbind(as.vector(con_glm1$byClass["Sensitivity"]), as.vector(con_glm2$byClass["Sensitivity"]), as.vector(con_lda$byClass["Sensitivity"]), as.vector(con_qda$byClass["Sensitivity"]), as.vector(con_knn3$byClass["Sensitivity"]))

rownames(sens) = colnames(con_glm1$byClass["Sensitivity"]); colnames(sens) = c("Logistic Reg","Logistic Reg (w. Training Set)", "LDA", "QDA", "KNN")
knitr::kable(sens, align = "c")
```

### Specificities for all Models

```{r spec all}
spec = cbind(as.vector(con_glm1$byClass["Specificity"]), as.vector(con_glm2$byClass["Specificity"]), as.vector(con_lda$byClass["Specificity"]), as.vector(con_qda$byClass["Specificity"]), as.vector(con_knn3$byClass["Specificity"]))

rownames(spec) = colnames(con_glm1$byClass["Specificity"]); colnames(spec) = c("Logistic Reg","Logistic Reg (w. Training Set)", "LDA", "QDA", "KNN")
knitr::kable(spec, align = "c")
```

By a large difference, the KNN model also had the highest sensitivity (0.5116), but this also meant it had the lowest specificity (0.5246). On the other hand, the model with the lowest sensitivity (0.1116) and highest specificity (0.9207) was the logistic model fitted with the full dataset. With all of these considerations, the KNN model with K = 3-Nearest Neighbors seems to be the optimal model from all of those tested for predicting the direction of the market using Lag 1 and Lag 2 as predictors. 

