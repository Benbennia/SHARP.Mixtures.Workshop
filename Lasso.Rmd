---
title: "Lasso"
author: "Ahlam Abuawad"
date: "7/12/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("janitor")
library(janitor)
#install.packages("caret")
library(caret)
#install.packages("Hmisc")
library(Hmisc)
#install.packages("glmnet")
library(glmnet)
```

# Importing and Cleaning Dataset

```{r}
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>% 
  clean_names(case = c("old_janitor")) %>% 
  mutate(bmi_cat3 = as.factor(bmi_cat3),
         edu_cat = as.factor(edu_cat),
         race_cat = as.factor(race_cat),
         male = as.factor(male)) 

# Checking Variables for Amount Missing, etc.
describe(study_pop)

# Removing Missings
data_lasso = study_pop %>% 
  select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>% 
  na.omit(telomean)

names(data_lasso)

# checking dimensions of dataset
dim(data_lasso)

# creating a matrix of predictors as x
x = model.matrix(telomean ~ ., data_lasso)[,-1]

my.x = names(data_lasso)[grep("la", names(data_lasso))]
length(my.x)

log.x = data.frame(apply(data_lasso[,my.x], 2, FUN = function(x) log(x)))
knitr::kable(names(log.x) <- paste(my.x, "l2", sep = "."))

dim(log.x)

summary(data_lasso[my.x])
summary(log.x)

# transforming outcome
y = data_lasso$telomean
lny = log(data_lasso$telomean)

# creating a df of confounders
covs = data_lasso[names(data_lasso)[-which(names(data_lasso) %in% c(my.x, "telomean", "seqn"))]]
names(covs)

# visualizing log-transformed x variables
featurePlot(x = log.x,
            y = lny,
           between = list(x = 1, y = 1), 
           type = c("g", "p", "smooth"))

# visualizing confounders
featurePlot(x = covs,
            y = lny,
           between = list(x = 1, y = 1), 
           type = c("g", "p", "smooth"))
```

# Lasso

```{r lasso}
# for reproducibility, set a seed
set.seed(2)

# n-folds is set to a default of 10 for cv.glmnet
# can include weights using "weights = ..."
conf = x[,19:36]

lasso = glmnet(log.x, lny, penalty.factor = c(0, rep(1, ncol(conf))), alpha = 1)
plot(lasso)

# use cross-validation to find best lambda value
cv.lasso = cv.glmnet(x, y, type.measure = "mse", alpha = 1)
plot(cv.lasso)
best_llambda = cv.lasso$lambda.min
best_llambda

# lasso model using best lambda value
lasso.mod = glmnet(x, y, penalty.factor = c(0, rep(1, ncol(conf))), alpha = 1, lambda = best_llambda)
coefficients_lasso = coef(lasso.mod)
coefficients_lasso

# MSE
lasso_mse = cv.lasso$lambda.1se
```

# Elastic Net

```{r}
# for reproducibility, set a seed
set.seed(2)

# n-folds is set to a default of 10 for cv.glmnet
elastic = glmnet(x, y, penalty.factor = c(0, rep(1, ncol(conf))), alpha = 0.5)
plot(elastic, xvar = "lambda")

# use cross-validation to find best lambda value
cv.elastic = cv.glmnet(x, y, type.measure = "mse", alpha = 0.5)
plot(cv.elastic)
best_elambda = cv.elastic$lambda.min
best_elambda

# elastic model using best lambda value
elastic.mod = glmnet(x, y, penalty.factor = c(0, rep(1, ncol(conf))), alpha = 0.5, lambda = best_elambda)
coefficients_elastic = coef(elastic.mod)
coefficients_elastic

# MSE
elastic_mse = cv.elastic$lambda.1se
```

# Comparing Lasso and Elastic Net

```{r}
# Table of MSE's
compare = cbind(as.vector(best_llambda), as.vector(lasso_mse), as.vector(best_elambda), as.vector(elastic_mse))
rownames(compare) = colnames(lasso_mse); colnames(compare) = c("Lasso Lambda", "Lasso Test MSE", "Elastic Lambda", "Elastic Net Test MSE")
knitr::kable(compare, align = "c")
```