---
title: "Lasso and Elastic Net"
author: "Ahlam Abuawad"
date: "7/12/2018"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("janitor")
library(janitor)
#install.packages("caret")
library(caret)
#install.packages("Hmisc")
library(Hmisc)
#install.packages("glmnet")
library(glmnet)
```

# Data Import and Cleaning 

```{r}
# Import dataset
study_pop = read_csv("./Data/studypop.csv") %>% 
  clean_names(case = c("old_janitor")) %>% 
  mutate(bmi_cat3 = as.factor(bmi_cat3),
         edu_cat = as.factor(edu_cat),
         race_cat = as.factor(race_cat),
         male = as.factor(male)) 

# Check variables for missing, etc.
describe(study_pop)

# Remove missing
data_lasso = study_pop %>% 
  dplyr::select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>% 
  na.omit(telomean)

names(data_lasso)

# Check dimensions of dataset
dim(data_lasso)

# Create a matrix of predictors as x
x = model.matrix(telomean ~ ., data_lasso)[,-1]

# ln transform within x matrix
x[,1:18] <- log(x[,1:18])

# Group pollutants
my.x = names(data_lasso)[grep("la", names(data_lasso))]
length(my.x)

# Transform outcome
y = data_lasso$telomean
lny = log(data_lasso$telomean)

# Group confounders
covs = data_lasso[names(data_lasso)[-which(names(data_lasso) %in% c(my.x, "telomean", "seqn"))]]
names(covs)
conf = x[,19:36]
```

## Data Visualization

```{r dataviz, warning = FALSE}
# Visualize log-transformed x variables
featurePlot(x = x[,1:18],
            y = lny,
            between = list(x = 1, y = 1), 
            type = c("g", "p", "smooth"))

# Visualize confounders
featurePlot(x = conf,
            y = lny,
            between = list(x = 1, y = 1),
            type = c("g", "p", "smooth"))
```

# Lasso w/ CV

```{r lasso}
# For reproducibility, set a seed
set.seed(2)

# n-folds is set to a default of 10 for cv.glmnet
## can include weights using "weights = ..."
## alpha is set to 1 due lasso's penalty
## use penalty.factor to retain certain variables in the model
lasso = glmnet(x, lny, 
               penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
               alpha = 1)
plot(lasso)

# Use cross-validation (CV) to find best lambda value
cv.lasso = cv.glmnet(x, lny, 
                     penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                     type.measure = "mse", alpha = 1)
plot(cv.lasso)

best_llambda = cv.lasso$lambda.min
best_llambda

# Lasso model using cross-validated lambda value
lasso.mod = glmnet(x, lny, 
                   penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                   alpha = 1, lambda = best_llambda)

coefficients_lasso = coef(lasso.mod)
coefficients_lasso

# Find the number of non-zero estimates
length(coefficients_lasso[coefficients_lasso != 0]) # 23 non-zero estimates

# Find the MSE
lasso_mse = cv.lasso$lambda.1se
lasso_mse
```

```{r combo_plot_lasso}
lasso_beta <- cbind(rownames(coefficients_lasso), as.vector(coefficients_lasso)) %>% 
  as.tibble() %>% 
  rename(variable = 1, beta = 2) %>% 
  mutate(beta = as.numeric(beta)) %>% 
  filter(variable != "(Intercept)") %>% 
  mutate(method = "Lasso")
```

# Elastic Net w/ CV

## Tune Alpha

```{r enalpha, results = "hide"}
# For reproducibility, set a seed
set.seed(2)

# Use CV to select best alpha value
## Create a tuning grid of alpha and lambda values
egrid <- expand.grid(alpha = (1:10) * 0.1, 
                     lambda = seq(0.01,0.4,0.01))

## Create a tuning control for cv
control <- trainControl(method = "repeatedcv", 
                        repeats = 3, 
                        verboseIter = TRUE)

## Use the tuning grid and control to find best alpha
elnetFit <- train(x = x, 
            y = lny,
            method = "glmnet",
            tuneGrid = egrid,
            trControl = control)
plot(elnetFit)

# The global minimum is 0.8 for alpha, thus it was chosen for the elastic net
```

## Tune Lambda and Coefficients

```{r elnet}
# For reproducibility, set a seed
set.seed(2)

# n-folds is set to a default of 10 for cv.glmnet
elnet = glmnet(x, lny, 
               penalty.factor = c(rep(1, ncol(x[,1:18])), 
                                  rep(0, ncol(x[,19:36]))),
               alpha = 0.8)
plot(elnet)

# Use CV to find best lambda value
cv.elnet = cv.glmnet(x, lny, 
                     penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),,
                     type.measure = "mse", alpha = 0.8)
plot(cv.elnet)

best_elambda = cv.elnet$lambda.min
best_elambda

# Elastic Net model using cross-validated alpha and lambda values
elnet.mod = glmnet(x, lny, 
                   penalty.factor = c(rep(1, ncol(x[,1:18])), rep(0, ncol(x[,19:36]))),
                   alpha = 0.8, lambda = best_elambda)

coefficients_elnet = coef(elnet.mod)
coefficients_elnet

# Find the number of non-zero estimates
length(coefficients_elnet[coefficients_elnet != 0]) # 24 non-zero estimates

# Find the MSE
elnet_mse = cv.elnet$lambda.1se
elnet_mse


```

```{r combo_plot_enet}
enet_beta <- cbind(rownames(coefficients_elnet), as.vector(coefficients_elnet)) %>% 
  as.tibble() %>% 
  rename(variable = 1, beta = 2) %>% 
  mutate(beta = as.numeric(beta)) %>% 
  filter(variable != "(Intercept)") %>% 
  mutate(method = "Elastic Net")
```

# Compare Lasso and Elastic Net

```{r compare}
# Table of lambda and MSE values for lasso and elastic net
compare = cbind(as.vector(best_llambda), as.vector(lasso_mse), as.vector(best_elambda), as.vector(elnet_mse))

rownames(compare) = colnames(lasso_mse); colnames(compare) = c("Lasso Lambda", "Lasso MSE", "Elastic Lambda", "Elastic Net MSE")

knitr::kable(compare, align = "c")
```

```{r create_4_plot}
ind <- rbind(enet_beta, lasso_beta)
write_csv(ind, "ind_betas.csv")
```