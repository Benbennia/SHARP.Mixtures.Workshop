y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# visualizing confounders
featurePlot(x = covs,
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
conf = x[,19:36]
# visualizing confounders
featurePlot(x = conf,
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# for reproducibility, set a seed
set.seed(2)
lasso = glmnet(log.x, lny, penalty.factor = c(0, rep(1, ncol(conf))), alpha = 1)
var = df(log.x,conf)
var = df(log.x, conf)
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("grpreg")
library(grpreg)
#install.packages("splines")
library(splines)
#install.packages("refund")
library(refund)
#install.packages("refund.shiny")
library(refund.shiny)
#install.packages("janitor")
library(janitor)
#This turns off scientific notation
options(scipen = 999)
install.packages("grpreg")
install.packages("refund")
install.packages("refund.shiny")
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("grpreg")
library(grpreg)
#install.packages("splines")
library(splines)
#install.packages("refund")
library(refund)
#install.packages("refund.shiny")
library(refund.shiny)
#install.packages("janitor")
library(janitor)
#This turns off scientific notation
options(scipen = 999)
data(Birthwt)
studypop <- read_csv("./Data/studypop.csv") %>% clean_names() %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male)) %>%
dplyr::select(-seqn)
#Group lasso wants data SEPARATE, with y value as one vector, X as a matrix of predictors (numeric values, categorical variables must be changed to 0/1 dummy variables), and grouping variable as a factor vector.
#Create X matrix of predictors with ln-transformed continuous variables (PCBs, Dioxins, and Furans) and dummy variables for categorical.
x <- model.matrix(telomean ~ ., studypop)[,-1]
#ln transform
x[,1:8] <- log(x[,1:8])
x[,15:24] <- log(x[,15:24])
#grpreg standardizes the data, so we don't need to
#These are complete cases
#y is the outcome variable
#ln transform
y <- log(na.omit(studypop)$telomean)
#Create grouping variable two ways. If there are coefficients to be included in the model without being penalized, assign them to group 0 (or "0"). This includes all covaruiates we want to keep in the model. Group must be a FACTOR.
# (1) 3 groups = 8 non-Dioxin-like PCBs, 2 non-ortho PCBs, and TEQ (3 dioxins, 4 furans, 1 mono-ortho (Dioxin-like) PCBs).
group3 <- vector()
group3[grepl("lbx1|0",colnames(x))] <- "Non-Dioxin-like PCB"
group3[grepl("lbxd", colnames(x))] <- "TEQ"
group3[grepl("lbxf", colnames(x))] <- "TEQ"
group3[grepl("lbxh|p", colnames(x))] <- "Non-Ortho PCB"
group3[grepl("lbx118la", colnames(x))] <- "TEQ"
group3[grepl("pct", colnames(x))] <- "0"
group3[grepl("bcsi", colnames(x))] <- "0"
group3[grepl("bmi", colnames(x))] <- "0"
group3[grepl("edu", colnames(x))] <- "0"
group3[grepl("race", colnames(x))] <- "0"
group3[grepl("male", colnames(x))] <- "0"
group3[grepl("bxcot", colnames(x))] <- "0"
group3[grepl("age", colnames(x))] <- "0"
group3 <- as.factor(group3)
cbind(colnames(x), group3)
# (2) 4 groups = 8 non-dioxin-like PCBs, 3 non-ortho PCBs, 3 dioxins, and 4 furans.
group4 <- vector()
group4[grepl("lbx1|0",colnames(x))] <- "Non-Dioxin-like PCB"
group4[grepl("lbxd", colnames(x))] <- "Dioxin"
group4[grepl("lbxf", colnames(x))] <- "Furan"
group4[grepl("lbxh|p", colnames(x))] <- "Non-Ortho PCB"
group4[grepl("lbx118la", colnames(x))] <- "Non-Ortho PCB"
group4[grepl("pct", colnames(x))] <- "0"
group4[grepl("bcsi", colnames(x))] <- "0"
group4[grepl("bmi", colnames(x))] <- "0"
group4[grepl("edu", colnames(x))] <- "0"
group4[grepl("race", colnames(x))] <- "0"
group4[grepl("male", colnames(x))] <- "0"
group4[grepl("bxcot", colnames(x))] <- "0"
group4[grepl("age", colnames(x))] <- "0"
group4 <- as.factor(group4)
cbind(colnames(x), group4)
lasso_3 <- grpreg(x, y, group3, penalty = "grLasso")
lasso_3
xvars = cbind(log.x, conf)
xvars
xvars
# for reproducibility, set a seed
set.seed(2)
# n-folds is set to a default of 10 for cv.glmnet
# can include weights using "weights = ..."
lasso = glmnet(xvars, lny, penalty.factor = c(1, rep(0, ncol(conf))), alpha = 1)
# n-folds is set to a default of 10 for cv.glmnet
# can include weights using "weights = ..."
lasso = glmnet(xvars, lny, penalty.factor = c(1, rep(0, ncol(xvars[,19:36]))), alpha = 1)
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male))
# Checking Variables for Amount Missing, etc.
describe(study_pop)
# Removing Missings
data_lasso = study_pop %>%
select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
names(data_lasso)
# checking dimensions of dataset
dim(data_lasso)
# creating a matrix of predictors as x
x = model.matrix(telomean ~ ., data_lasso)[,-1]
my.x = names(data_lasso)[grep("la", names(data_lasso))]
length(my.x)
log.x = data.frame(apply(data_lasso[,my.x], 2, FUN = function(x) log(x)))
knitr::kable(names(log.x) <- paste(my.x, "l2", sep = "."))
dim(log.x)
summary(data_lasso[my.x])
summary(log.x)
# transforming outcome
y = data_lasso$telomean
lny = log(data_lasso$telomean)
# creating a df of confounders
covs = data_lasso[names(data_lasso)[-which(names(data_lasso) %in% c(my.x, "telomean", "seqn"))]]
names(covs)
conf = x[,19:36]
# combining log-transformed x variables and confounders into one dataframe
xvars = cbind(log.x, conf)
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male))
# Checking Variables for Amount Missing, etc.
describe(study_pop)
# Removing Missings
data_lasso = study_pop %>%
select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male))
# Checking Variables for Amount Missing, etc.
describe(study_pop)
# Removing Missings
data_lasso = study_pop %>%
select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male)) %>%
select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
study_pop = read_csv("./Data/studypop.csv")
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male))
View(study_pop)
# Removing Missings
data_lasso = study_pop %>%
select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
# Removing Missings
data_lasso = study_pop %>%
dplyr::select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
names(data_lasso)
# checking dimensions of dataset
dim(data_lasso)
# creating a matrix of predictors as x
x = model.matrix(telomean ~ ., data_lasso)[,-1]
my.x = names(data_lasso)[grep("la", names(data_lasso))]
length(my.x)
log.x = data.frame(apply(data_lasso[,my.x], 2, FUN = function(x) log(x)))
knitr::kable(names(log.x) <- paste(my.x, "l2", sep = "."))
dim(log.x)
summary(data_lasso[my.x])
summary(log.x)
# transforming outcome
y = data_lasso$telomean
lny = log(data_lasso$telomean)
# creating a df of confounders
covs = data_lasso[names(data_lasso)[-which(names(data_lasso) %in% c(my.x, "telomean", "seqn"))]]
names(covs)
conf = x[,19:36]
# combining log-transformed x variables and confounders into one dataframe
xvars = cbind(log.x, conf)
# for reproducibility, set a seed
set.seed(2)
# n-folds is set to a default of 10 for cv.glmnet
# can include weights using "weights = ..."
lasso = glmnet(xvars, lny, penalty.factor = c(1, rep(0, ncol(xvars[,19:36]))), alpha = 1)
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male))
# Removing Missings
data_lasso = study_pop %>%
dplyr::select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
# creating a matrix of predictors as x
x = model.matrix(telomean ~ ., data_lasso)[,-1]
#ln transform
x[,1:8] <- log(x[,1:8])
x[,15:24] <- log(x[,15:24])
x
summary(data_lasso$x)
summary(x)
# creating a df of confounders
covs = data_lasso[names(data_lasso)[-which(names(data_lasso) %in% c(x, "telomean", "seqn"))]]
names(covs)
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male))
# Checking Variables for Amount Missing, etc.
describe(study_pop)
# Removing Missings
data_lasso = study_pop %>%
dplyr::select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
names(data_lasso)
# checking dimensions of dataset
dim(data_lasso)
# creating a matrix of predictors as x
x = model.matrix(telomean ~ ., data_lasso)[,-1]
my.x = names(data_lasso)[grep("la", names(data_lasso))]
length(my.x)
log.x = data.frame(apply(data_lasso[,my.x], 2, FUN = function(x) log(x)))
knitr::kable(names(log.x) <- paste(my.x, "l2", sep = "."))
dim(log.x)
summary(data_lasso[my.x])
summary(log.x)
# transforming outcome
y = data_lasso$telomean
lny = log(data_lasso$telomean)
# creating a df of confounders
covs = data_lasso[names(data_lasso)[-which(names(data_lasso) %in% c(my.x, "telomean", "seqn"))]]
names(covs)
conf = x[,19:36]
# combining log-transformed x variables and confounders into one dataframe
xvars = cbind(log.x, conf)
xvars
# for reproducibility, set a seed
set.seed(2)
# n-folds is set to a default of 10 for cv.glmnet
# can include weights using "weights = ..."
lasso = glmnet(xvars, lny, penalty.factor = c(1, rep(0, ncol(xvars[,19:36]))), alpha = 1)
# visualizing log-transformed x variables
featurePlot(x = log.x,
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# visualizing confounders
featurePlot(x = conf,
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# combining log-transformed x variables and confounders into one dataframe
xvars = df(cbind(log.x, conf))
# combining log-transformed x variables and confounders into one dataframe
xvars = data.frame(cbind(log.x, conf))
xvars
# n-folds is set to a default of 10 for cv.glmnet
# can include weights using "weights = ..."
lasso = glmnet(xvars, lny, penalty.factor = c(1, rep(0, ncol(xvars[,19:36]))), alpha = 1)
studypop <- read_csv("./Data/studypop.csv") %>% clean_names() %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male)) %>%
dplyr::select(-seqn)
#Create X matrix of predictors with ln-transformed continuous variables (PCBs, Dioxins, and Furans) and dummy variables for categorical.
x <- model.matrix(telomean ~ ., studypop)[,-1]
x
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male))
# Checking Variables for Amount Missing, etc.
describe(study_pop)
# Removing Missings
data_lasso = study_pop %>%
dplyr::select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
names(data_lasso)
# checking dimensions of dataset
dim(data_lasso)
# creating a matrix of predictors as x
x = model.matrix(telomean ~ ., data_lasso)[,-1]
x
# ln transform
x[,1:18] <- log(x[,1:18])
x[,1:18]
# n-folds is set to a default of 10 for cv.glmnet
# can include weights using "weights = ..."
lasso = glmnet(x, lny, penalty.factor = c(1, rep(0, ncol(xvars[,19:36]))), alpha = 1)
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male))
# Checking Variables for Amount Missing, etc.
describe(study_pop)
# Removing Missings
data_lasso = study_pop %>%
dplyr::select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
names(data_lasso)
# checking dimensions of dataset
dim(data_lasso)
# creating a matrix of predictors as x
x = model.matrix(telomean ~ ., data_lasso)[,-1]
# ln transform within x matrix
x[,1:18] <- log(x[,1:18])
# grouping pollutants
my.x = names(data_lasso)[grep("la", names(data_lasso))]
length(my.x)
# transforming outcome
y = data_lasso$telomean
lny = log(data_lasso$telomean)
# grouping confounders
covs = data_lasso[names(data_lasso)[-which(names(data_lasso) %in% c(my.x, "telomean", "seqn"))]]
names(covs)
conf = x[,19:36]
# visualizing log-transformed x variables
featurePlot(x = x,
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# visualizing confounders
featurePlot(x = conf,
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# visualizing log-transformed x variables
featurePlot(x = x[,1:18],
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# for reproducibility, set a seed
set.seed(2)
# n-folds is set to a default of 10 for cv.glmnet
# can include weights using "weights = ..."
lasso = glmnet(x, lny, penalty.factor = c(1, rep(0, ncol(x[,19:36]))), alpha = 1)
plot(lasso)
plot(lasso, xvar = "lambda")
# use cross-validation to find best lambda value
cv.lasso = cv.glmnet(x, y, type.measure = "mse", alpha = 1)
plot(cv.lasso)
best_llambda = cv.lasso$lambda.min
best_llambda
# lasso model using best lambda value
lasso.mod = glmnet(x, y, penalty.factor = c(1, rep(0, ncol(conf))), alpha = 1, lambda = best_llambda)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("janitor")
library(janitor)
#install.packages("caret")
library(caret)
#install.packages("Hmisc")
library(Hmisc)
#install.packages("glmnet")
library(glmnet)
# Chunk 2
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male))
# Checking Variables for Amount Missing, etc.
describe(study_pop)
# Removing Missings
data_lasso = study_pop %>%
dplyr::select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
names(data_lasso)
# checking dimensions of dataset
dim(data_lasso)
# creating a matrix of predictors as x
x = model.matrix(telomean ~ ., data_lasso)[,-1]
# ln transform within x matrix
x[,1:18] <- log(x[,1:18])
# grouping pollutants
my.x = names(data_lasso)[grep("la", names(data_lasso))]
length(my.x)
# transforming outcome
y = data_lasso$telomean
lny = log(data_lasso$telomean)
# grouping confounders
covs = data_lasso[names(data_lasso)[-which(names(data_lasso) %in% c(my.x, "telomean", "seqn"))]]
names(covs)
conf = x[,19:36]
# Chunk 3: dataviz
# visualizing log-transformed x variables
featurePlot(x = x[,1:18],
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# visualizing confounders
featurePlot(x = conf,
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# Chunk 4: lasso
# for reproducibility, set a seed
set.seed(2)
# n-folds is set to a default of 10 for cv.glmnet
# can include weights using "weights = ..."
lasso = glmnet(x, lny, penalty.factor = c(1, rep(0, ncol(x[,19:36]))), alpha = 1)
plot(lasso, xvar = "lambda")
# use cross-validation to find best lambda value
cv.lasso = cv.glmnet(x, y, type.measure = "mse", alpha = 1)
plot(cv.lasso)
best_llambda = cv.lasso$lambda.min
best_llambda
# lasso model using best lambda value
lasso.mod = glmnet(x, y, penalty.factor = c(1, rep(0, ncol(x[,19:36]))), alpha = 1, lambda = best_llambda)
coefficients_lasso = coef(lasso.mod)
coefficients_lasso
# MSE
lasso_mse = cv.lasso$lambda.1se
# Chunk 5
# for reproducibility, set a seed
set.seed(2)
# n-folds is set to a default of 10 for cv.glmnet
elastic = glmnet(x, y, penalty.factor = c(1, rep(0, ncol(x[,19:36]))), alpha = 0.5)
plot(elastic, xvar = "lambda")
# use cross-validation to find best lambda value
cv.elastic = cv.glmnet(x, y, type.measure = "mse", alpha = 0.5)
plot(cv.elastic)
best_elambda = cv.elastic$lambda.min
best_elambda
# elastic model using best lambda value
elastic.mod = glmnet(x, y, penalty.factor = c(1, rep(0, ncol(x[,19:36]))), alpha = 0.5, lambda = best_elambda)
coefficients_elastic = coef(elastic.mod)
coefficients_elastic
# MSE
elastic_mse = cv.elastic$lambda.1se
# Chunk 6
# Table of MSE's
compare = cbind(as.vector(best_llambda), as.vector(lasso_mse), as.vector(best_elambda), as.vector(elastic_mse))
rownames(compare) = colnames(lasso_mse); colnames(compare) = c("Lasso Lambda", "Lasso Test MSE", "Elastic Lambda", "Elastic Net Test MSE")
knitr::kable(compare, align = "c")
