#install.packages("caret")
library(caret)
#install.packages("Hmisc")
library(Hmisc)
#install.packages("glmnet")
library(glmnet)
# Chunk 2
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male))
# Checking Variables for Amount Missing, etc.
describe(study_pop)
# Removing Missings
data_lasso = study_pop %>%
dplyr::select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
names(data_lasso)
# checking dimensions of dataset
dim(data_lasso)
# creating a matrix of predictors as x
x = model.matrix(telomean ~ ., data_lasso)[,-1]
# ln transform within x matrix
x[,1:18] <- log(x[,1:18])
# grouping pollutants
my.x = names(data_lasso)[grep("la", names(data_lasso))]
length(my.x)
# transforming outcome
y = data_lasso$telomean
lny = log(data_lasso$telomean)
# grouping confounders
covs = data_lasso[names(data_lasso)[-which(names(data_lasso) %in% c(my.x, "telomean", "seqn"))]]
names(covs)
conf = x[,19:36]
# Chunk 3: dataviz
# visualizing log-transformed x variables
featurePlot(x = x[,1:18],
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# visualizing confounders
featurePlot(x = conf,
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# Chunk 4: lasso
# for reproducibility, set a seed
set.seed(2)
# n-folds is set to a default of 10 for cv.glmnet
# can include weights using "weights = ..."
lasso = glmnet(x, lny, penalty.factor = c(1, rep(0, ncol(x[,19:36]))), alpha = 1)
plot(lasso, xvar = "lambda")
# use cross-validation to find best lambda value
cv.lasso = cv.glmnet(x, y, type.measure = "mse", alpha = 1)
plot(cv.lasso)
best_llambda = cv.lasso$lambda.min
best_llambda
# lasso model using best lambda value
lasso.mod = glmnet(x, y, penalty.factor = c(1, rep(0, ncol(x[,19:36]))), alpha = 1, lambda = best_llambda)
coefficients_lasso = coef(lasso.mod)
coefficients_lasso
# MSE
lasso_mse = cv.lasso$lambda.1se
# Chunk 5
# for reproducibility, set a seed
set.seed(2)
# n-folds is set to a default of 10 for cv.glmnet
elastic = glmnet(x, y, penalty.factor = c(1, rep(0, ncol(x[,19:36]))), alpha = 0.5)
plot(elastic, xvar = "lambda")
# use cross-validation to find best lambda value
cv.elastic = cv.glmnet(x, y, type.measure = "mse", alpha = 0.5)
plot(cv.elastic)
best_elambda = cv.elastic$lambda.min
best_elambda
# elastic model using best lambda value
elastic.mod = glmnet(x, y, penalty.factor = c(1, rep(0, ncol(x[,19:36]))), alpha = 0.5, lambda = best_elambda)
coefficients_elastic = coef(elastic.mod)
coefficients_elastic
# MSE
elastic_mse = cv.elastic$lambda.1se
# Chunk 6
# Table of MSE's
compare = cbind(as.vector(best_llambda), as.vector(lasso_mse), as.vector(best_elambda), as.vector(elastic_mse))
rownames(compare) = colnames(lasso_mse); colnames(compare) = c("Lasso Lambda", "Lasso Test MSE", "Elastic Lambda", "Elastic Net Test MSE")
knitr::kable(compare, align = "c")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("janitor")
library(janitor)
#install.packages("caret")
library(caret)
#install.packages("Hmisc")
library(Hmisc)
#install.packages("glmnet")
library(glmnet)
# Chunk 2
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male))
# Checking Variables for Amount Missing, etc.
describe(study_pop)
# Removing Missings
data_lasso = study_pop %>%
dplyr::select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
names(data_lasso)
# checking dimensions of dataset
dim(data_lasso)
# creating a matrix of predictors as x
x = model.matrix(telomean ~ ., data_lasso)[,-1]
# ln transform within x matrix
x[,1:18] <- log(x[,1:18])
# grouping pollutants
my.x = names(data_lasso)[grep("la", names(data_lasso))]
length(my.x)
# transforming outcome
y = data_lasso$telomean
lny = log(data_lasso$telomean)
# grouping confounders
covs = data_lasso[names(data_lasso)[-which(names(data_lasso) %in% c(my.x, "telomean", "seqn"))]]
names(covs)
conf = x[,19:36]
# Chunk 3: dataviz
# visualizing log-transformed x variables
featurePlot(x = x[,1:18],
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# visualizing confounders
featurePlot(x = conf,
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# Chunk 4: lasso
# for reproducibility, set a seed
set.seed(2)
# n-folds is set to a default of 10 for cv.glmnet
# can include weights using "weights = ..."
lasso = glmnet(x, lny, penalty.factor = c(0, rep(1, ncol(x[,19:36]))), alpha = 1)
plot(lasso)
# use cross-validation to find best lambda value
cv.lasso = cv.glmnet(x, y, type.measure = "mse", alpha = 1)
plot(cv.lasso)
best_llambda = cv.lasso$lambda.min
best_llambda
# lasso model using best lambda value
lasso.mod = glmnet(x, y, penalty.factor = c(0, rep(1, ncol(x[,19:36]))), alpha = 1, lambda = best_llambda)
coefficients_lasso = coef(lasso.mod)
coefficients_lasso
# MSE
lasso_mse = cv.lasso$lambda.1se
# Chunk 5: elastic
# for reproducibility, set a seed
set.seed(2)
# n-folds is set to a default of 10 for cv.glmnet
elastic = glmnet(x, lny, penalty.factor = c(0, rep(1, ncol(x[,19:36]))), alpha = 0.5)
plot(elastic)
# use cross-validation to find best lambda value
cv.elastic = cv.glmnet(x, lny, type.measure = "mse", alpha = 0.5)
plot(cv.elastic)
best_elambda = cv.elastic$lambda.min
best_elambda
# elastic model using best lambda value
elastic.mod = glmnet(x, lny, penalty.factor = c(0, rep(1, ncol(x[,19:36]))), alpha = 0.5, lambda = best_elambda)
coefficients_elastic = coef(elastic.mod)
coefficients_elastic
# MSE
elastic_mse = cv.elastic$lambda.1se
# Chunk 6: compare
# Table of MSE's
compare = cbind(as.vector(best_llambda), as.vector(lasso_mse), as.vector(best_elambda), as.vector(elastic_mse))
rownames(compare) = colnames(lasso_mse); colnames(compare) = c("Lasso Lambda", "Lasso MSE", "Elastic Lambda", "Elastic Net MSE")
knitr::kable(compare, align = "c")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("janitor")
library(janitor)
#install.packages("caret")
library(caret)
#install.packages("Hmisc")
library(Hmisc)
#install.packages("glmnet")
library(glmnet)
# Chunk 2
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male))
# Checking Variables for Amount Missing, etc.
describe(study_pop)
# Removing Missings
data_lasso = study_pop %>%
dplyr::select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
names(data_lasso)
# checking dimensions of dataset
dim(data_lasso)
# creating a matrix of predictors as x
x = model.matrix(telomean ~ ., data_lasso)[,-1]
# ln transform within x matrix
x[,1:18] <- log(x[,1:18])
# grouping pollutants
my.x = names(data_lasso)[grep("la", names(data_lasso))]
length(my.x)
# transforming outcome
y = data_lasso$telomean
lny = log(data_lasso$telomean)
# grouping confounders
covs = data_lasso[names(data_lasso)[-which(names(data_lasso) %in% c(my.x, "telomean", "seqn"))]]
names(covs)
conf = x[,19:36]
# Chunk 3: dataviz
# visualizing log-transformed x variables
featurePlot(x = x[,1:18],
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# visualizing confounders
featurePlot(x = conf,
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# Chunk 4: lasso
# for reproducibility, set a seed
set.seed(2)
# n-folds is set to a default of 10 for cv.glmnet
# can include weights using "weights = ..."
lasso = glmnet(x, lny, penalty.factor = c(0, rep(1, ncol(x[,19:36]))), alpha = 1)
plot(lasso)
# use cross-validation to find best lambda value
cv.lasso = cv.glmnet(x, lny, type.measure = "mse", alpha = 1)
plot(cv.lasso)
best_llambda = cv.lasso$lambda.min
best_llambda
# lasso model using best lambda value
lasso.mod = glmnet(x, lny, penalty.factor = c(0, rep(1, ncol(x[,19:36]))), alpha = 1, lambda = best_llambda)
coefficients_lasso = coef(lasso.mod)
coefficients_lasso
# MSE
lasso_mse = cv.lasso$lambda.1se
# Chunk 5: elastic
# for reproducibility, set a seed
set.seed(2)
# n-folds is set to a default of 10 for cv.glmnet
elastic = glmnet(x, lny, penalty.factor = c(0, rep(1, ncol(x[,19:36]))), alpha = 0.5)
plot(elastic)
# use cross-validation to find best lambda value
cv.elastic = cv.glmnet(x, lny, type.measure = "mse", alpha = 0.5)
plot(cv.elastic)
best_elambda = cv.elastic$lambda.min
best_elambda
# elastic model using best lambda value
elastic.mod = glmnet(x, lny, penalty.factor = c(0, rep(1, ncol(x[,19:36]))), alpha = 0.5, lambda = best_elambda)
coefficients_elastic = coef(elastic.mod)
coefficients_elastic
# MSE
elastic_mse = cv.elastic$lambda.1se
# Chunk 6: compare
# Table of MSE's
compare = cbind(as.vector(best_llambda), as.vector(lasso_mse), as.vector(best_elambda), as.vector(elastic_mse))
rownames(compare) = colnames(lasso_mse); colnames(compare) = c("Lasso Lambda", "Lasso MSE", "Elastic Lambda", "Elastic Net MSE")
knitr::kable(compare, align = "c")
---
title: "Lasso"
author: "Ahlam Abuawad"
date: "7/12/2018"
output:
html_document:
toc: TRUE
toc_float: TRUE
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("janitor")
library(janitor)
#install.packages("caret")
library(caret)
#install.packages("Hmisc")
library(Hmisc)
#install.packages("glmnet")
library(glmnet)
```
# Importing and Cleaning Dataset
```{r}
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male))
# Checking Variables for Amount Missing, etc.
describe(study_pop)
# Removing Missings
data_lasso = study_pop %>%
dplyr::select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
names(data_lasso)
# checking dimensions of dataset
dim(data_lasso)
# creating a matrix of predictors as x
x = model.matrix(telomean ~ ., data_lasso)[,-1]
# ln transform within x matrix
x[,1:18] <- log(x[,1:18])
# grouping pollutants
my.x = names(data_lasso)[grep("la", names(data_lasso))]
length(my.x)
# transforming outcome
y = data_lasso$telomean
lny = log(data_lasso$telomean)
# grouping confounders
covs = data_lasso[names(data_lasso)[-which(names(data_lasso) %in% c(my.x, "telomean", "seqn"))]]
names(covs)
conf = x[,19:36]
```
## Data Visualization
```{r dataviz, warning = FALSE}
# visualizing log-transformed x variables
featurePlot(x = x[,1:18],
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# visualizing confounders
featurePlot(x = conf,
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
```
# Lasso
```{r lasso}
# for reproducibility, set a seed
set.seed(2)
# n-folds is set to a default of 10 for cv.glmnet
# can include weights using "weights = ..."
lasso = glmnet(x, lny, penalty.factor = c(0, rep(1, ncol(x[,19:36]))), alpha = 1)
plot(lasso)
# use cross-validation to find best lambda value
cv.lasso = cv.glmnet(x, lny, type.measure = "mse", alpha = 1)
plot(cv.lasso)
best_llambda = cv.lasso$lambda.min
best_llambda
# lasso model using best lambda value
lasso.mod = glmnet(x, lny, penalty.factor = c(0, rep(1, ncol(x[,19:36]))), alpha = 1, lambda = best_llambda)
coefficients_lasso = coef(lasso.mod)
coefficients_lasso
# MSE
lasso_mse = cv.lasso$lambda.1se
```
# Elastic Net
```{r elnet}
# for reproducibility, set a seed
set.seed(2)
# use cross-validation to find the best alpha
a <- seq(0, 1, length = 10)
trnctrl = trainControl(
method = "repeatedCV",
number = 10,
repeats = 5
)
pol_elnet = train(x ~ ., data = data_lasso,
method = "glmnet",
trControl = trnctrl
)
# Importing Dataset
study_pop = read_csv("./Data/studypop.csv") %>%
clean_names(case = c("old_janitor")) %>%
mutate(bmi_cat3 = as.factor(bmi_cat3),
edu_cat = as.factor(edu_cat),
race_cat = as.factor(race_cat),
male = as.factor(male))
# Removing Missings
data_lasso = study_pop %>%
dplyr::select(telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn) %>%
na.omit(telomean)
names(data_lasso)
# checking dimensions of dataset
dim(data_lasso)
# creating a matrix of predictors as x
x = model.matrix(telomean ~ ., data_lasso)[,-1]
# ln transform within x matrix
x[,1:18] <- log(x[,1:18])
# grouping pollutants
my.x = names(data_lasso)[grep("la", names(data_lasso))]
length(my.x)
# transforming outcome
y = data_lasso$telomean
lny = log(data_lasso$telomean)
# grouping confounders
covs = data_lasso[names(data_lasso)[-which(names(data_lasso) %in% c(my.x, "telomean", "seqn"))]]
names(covs)
conf = x[,19:36]
# visualizing log-transformed x variables
featurePlot(x = x[,1:18],
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# visualizing confounders
featurePlot(x = conf,
y = lny,
between = list(x = 1, y = 1),
type = c("g", "p", "smooth"))
# for reproducibility, set a seed
set.seed(2)
# n-folds is set to a default of 10 for cv.glmnet
# can include weights using "weights = ..."
lasso = glmnet(x, lny, penalty.factor = c(0, rep(1, ncol(x[,19:36]))), alpha = 1)
plot(lasso)
# use cross-validation to find best lambda value
cv.lasso = cv.glmnet(x, lny, type.measure = "mse", alpha = 1)
plot(cv.lasso)
best_llambda = cv.lasso$lambda.min
best_llambda
# lasso model using best lambda value
lasso.mod = glmnet(x, lny, penalty.factor = c(0, rep(1, ncol(x[,19:36]))), alpha = 1, lambda = best_llambda)
coefficients_lasso = coef(lasso.mod)
coefficients_lasso
# MSE
lasso_mse = cv.lasso$lambda.1se
# for reproducibility, set a seed
set.seed(2)
trnctrl = trainControl(
method = "repeatedCV",
number = 10,
repeats = 5
)
# use cross-validation to find the best alpha
trnctrl = trainControl(
method = "repeatedCV",
number = 10
)
pol_elnet = train(x ~ ., data = data_lasso,
method = "glmnet",
trControl = trnctrl
)
pol_elnet = train(x ~ lny, data = data_lasso,
method = "glmnet",
trControl = trnctrl
)
# use cross-validation to find the best alpha
trnctrl = trainControl(method = "cv",
number = 10)
pol_elnet = train(x ~ lny, data = data_lasso,
method = "glmnet",
trControl = trnctrl
)
pol_elnet = train(x ~ ., data = data_lasso,
method = "glmnet",
trControl = trnctrl
)
elnet_alpha = train(lny ~ ., data = data_lasso,
method = "glmnet",
trControl = trnctrl
)
# use cross-validation to find the best alpha
trnctrl = trainControl(method = "cv",
number = 10
)
elnet_alpha = train(lny ~ ., data = data_lasso,
method = "glmnet",
trControl = trnctrl
)
trnctrl = trainControl(method = "cv",
number = 10
)
tuneGrid <- expand.grid(alpha = 0:1,
lambda = seq(0.0001, 1, length = 10)
)
# Fit a model
model <- train(lny ~ ., data_lasso, method = "glmnet",
tuneGrid = tuneGrid, trControl = trnctrl
)
# use cross-validation to find the best alpha
lambda_grid <- seq(0, 1000, length = 100)
alpha_grid <- seq(0, 1, length = 10)
trnCtrl <- trainControl(method = "repeatedCV",
number = 10,
repeats = 5)
trnCtrl <- trainControl(method = "repeatedCV",
number = 10
)
srchGrid <- expand.grid(.alpha = alpha_grid, .lambda = lambda_grid)
# Cross validation
my_train <- train(lny ~., data_lasso,
method = "glmnet",
tuneGrid = srchGrid,
trControl = trnCtrl)
alpha <- seq(0,1, by = 0.1)
elnet_alpha <- lapply(alpha, function(a){
cv.glmnet(x, lny, alpha = a, family = "binomial", lambda.min.ratio = 0.001)
})
elnet_alpha <- lapply(alpha, function(a){
cv.glmnet(x, lny, alpha = a, family = "binomial", lambda.min.ratio = 0.001)
})
alpha <- seq(0,1, by = 0.1)
# n-folds is set to a default of 10 for cv.glmnet
elnet = glmnet(x, lny, penalty.factor = c(0, rep(1, ncol(x[,19:36]))), alpha = alpha)
